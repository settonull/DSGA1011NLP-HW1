{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DS-GA 1011 HW1\n",
    "#Author: Chris Rogers\n",
    "#Code adapted from DS-GA 1011 Lab 3\n",
    "\n",
    "\n",
    "#load all of the reviews\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "#start out the same very time \n",
    "random.seed(12345)\n",
    "\n",
    "#should we run each function as we define it, mostly used for testing initially\n",
    "run_inline = False\n",
    "\n",
    "#assumes the aclImdb directory is present locally\n",
    "root_dir = \"aclImdb\"\n",
    "train_dir = \"train\"\n",
    "test_dir = \"test\"\n",
    "pos_dir = \"pos\"\n",
    "neg_dir = \"neg\"\n",
    "\n",
    "#split our \"train\" set 20000/5000 with validation\n",
    "train_split = 20000\n",
    "\n",
    "scheduler = None\n",
    "\n",
    "def loadDirectory(reviewList, directory):\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        with (open(os.path.join(directory, file), \"r\", encoding=\"utf-8\")) as review_file:\n",
    "            reviewList.append(review_file.read())\n",
    "\n",
    "            \n",
    "train_data = []\n",
    "loadDirectory(train_data,os.path.join(root_dir, train_dir, pos_dir))\n",
    "train_target_data = [1]*len(train_data)\n",
    "loadDirectory(train_data,os.path.join(root_dir, train_dir, neg_dir))\n",
    "train_target_data += ([0]*(len(train_data) - len(train_target_data)))\n",
    "\n",
    "test_data = []\n",
    "loadDirectory(test_data,os.path.join(root_dir, test_dir, pos_dir))\n",
    "test_target_data = [1]*len(test_data)\n",
    "loadDirectory(test_data,os.path.join(root_dir, test_dir, neg_dir))\n",
    "test_target_data += ([0]*(len(test_data) - len(test_target_data)))\n",
    "    \n",
    "combined = list(zip(train_data, train_target_data ))\n",
    "random.shuffle(combined)\n",
    "train_data[:], train_target_data[:] = zip(*combined)\n",
    "\n",
    "combined = list(zip(test_data, test_target_data ))\n",
    "random.shuffle(combined)\n",
    "test_data[:], test_target_data[:] = zip(*combined)\n",
    "\n",
    "\n",
    "val_reviews = train_data[train_split:]\n",
    "train_reviews = train_data[:train_split]\n",
    "\n",
    "\n",
    "val_targets = np.array(train_target_data[train_split:])\n",
    "train_targets = np.array(train_target_data[:train_split])\n",
    "\n",
    "\n",
    "#a bit redundant, but was less confusing to keep the naming the same above\n",
    "test_reviews = test_data\n",
    "test_targets = np.array(test_target_data)\n",
    "    \n",
    "print(\"train reviews:\", len(train_reviews))\n",
    "print(\"train targets:\", len(train_targets))\n",
    "print(\"val reviews:\", len(val_reviews))\n",
    "print(\"val targets:\", len(val_targets))\n",
    "print(\"test reviews:\", len(test_reviews)) \n",
    "print(\"test targets:\", len(test_targets)) \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create a tokenized version of a string, with the specified pre-processing and n-grams\n",
    "def tokenize(string_list, removePunct = True, removeNumbers=False, replaceNumbers=True, toLower=True, ngrams=1):\n",
    "    \n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    remove_punct = str.maketrans('', '', string.punctuation)\n",
    "    token_list = []\n",
    "    \n",
    "    for string_val in string_list:\n",
    "        if (toLower) :\n",
    "            string_val = string_val.lower()\n",
    "            \n",
    "        if (removeNumbers) :\n",
    "            string_val = string_val.translate(remove_digits)\n",
    "        \n",
    "        if (removePunct) :\n",
    "            string_val = string_val.translate(remove_punct)\n",
    "        \n",
    "        tokens = nltk.word_tokenize(string_val)\n",
    "        \n",
    "        if (replaceNumbers):\n",
    "            tokens = [ token if not token.isdigit() else 'NMBR' for token in tokens]\n",
    "            \n",
    "        full_tokens = tokens\n",
    "        while (ngrams > 1):\n",
    "            full_tokens += list(nltk.ngrams(tokens, ngrams))\n",
    "            ngrams -= 1\n",
    "            \n",
    "        token_list.append(full_tokens)\n",
    "        \n",
    "    \n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the given set of pre-processing, go through and tokenize our three datasets\n",
    "def tokenize_data(td_replaceNumbers=True, td_removeNumbers = False, td_toLower=True, td_removePunct=True, td_ngrams=1):\n",
    "    train_tokens = tokenize(train_reviews, replaceNumbers=td_replaceNumbers, toLower=td_toLower, \n",
    "                            removePunct=td_removePunct, ngrams=td_ngrams, removeNumbers=td_removeNumbers)\n",
    "    val_tokens = tokenize(val_reviews, replaceNumbers=td_replaceNumbers, toLower=td_toLower, \n",
    "                            removePunct=td_removePunct, ngrams=td_ngrams,  removeNumbers=td_removeNumbers)\n",
    "    test_tokens = tokenize(test_reviews, replaceNumbers=td_replaceNumbers, toLower=td_toLower, \n",
    "                            removePunct=td_removePunct, ngrams=td_ngrams,  removeNumbers=td_removeNumbers)\n",
    "\n",
    "    all_train_tokens = []\n",
    "\n",
    "    for toklist in train_tokens:\n",
    "        all_train_tokens.extend(toklist)\n",
    "        \n",
    "    return train_tokens, val_tokens, test_tokens, all_train_tokens\n",
    "\n",
    "if (run_inline):\n",
    "    train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data()\n",
    "    #taking a look, \n",
    "    token_counter = Counter(all_train_tokens)\n",
    "    token_counter.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we'll use our max vocab size to build an indexed vocabulary\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size = 30000):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "if (run_inline):\n",
    "    token2id, id2token = build_vocab(all_train_tokens)\n",
    "\n",
    "    # Lets check the dictionary by loading random token from it\n",
    "\n",
    "    random_token_id = random.randint(0, len(id2token)-1)\n",
    "    random_token = id2token[random_token_id]\n",
    "\n",
    "    print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "    print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "if (run_inline):\n",
    "    train_data_indices = token2index_dataset(train_tokens)\n",
    "    val_data_indices = token2index_dataset(val_tokens)\n",
    "    test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "    # double checking\n",
    "    print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "    print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "    print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "    print(train_data[0])\n",
    "    print(train_data_indices[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our dataset for our investigations\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list, max_sentenance_length):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        self.max_sentenance_length = max_sentenance_length\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:self.max_sentenance_length]\n",
    "        token_len = len(token_idx)\n",
    "        \n",
    "        token_idx = np.pad(np.array(token_idx), \n",
    "                                pad_width=((0,self.max_sentenance_length-token_len)), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        \n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, token_len, label]\n",
    "\n",
    "        \n",
    "if (run_inline):\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_SENTENANCE_LENGTH = 400\n",
    "    train_dataset = NewsGroupDataset(train_data_indices, train_targets, MAX_SENTENANCE_LENGTH)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = NewsGroupDataset(val_data_indices, val_targets, MAX_SENTENANCE_LENGTH)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = NewsGroupDataset(test_data_indices, test_targets, MAX_SENTENANCE_LENGTH)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#this defines our bag of words model, which uses a linear combination of embeddings\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data.long())\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        labels = labels.long()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct = correct + predicted.long().eq(labels.view_as(predicted.long())).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "def run_model(verbose = True, num_epochs = 10):\n",
    "    results = []\n",
    "    for epoch in range(num_epochs):\n",
    "        if (scheduler):\n",
    "            scheduler.step()\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels.long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 300 == 0 and verbose:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                               epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "        train_acc = test_model(train_loader, model)\n",
    "        val_acc = test_model(val_loader, model)\n",
    "        results.append([train_acc,val_acc])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#Display a chart of the data, given the label, a list of runs, and the number of \n",
    "#columns to display the legend.\n",
    "#assumes data is in {paramaters1:[[E1-trainacc,E1-valacc],[E2-trainacc,E2-valacc]...]}\n",
    "def show_chart(plot_label, res_run, cols):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    for key in res_run:\n",
    "        arun = np.array(res_run[key])\n",
    "        y = arun[:,0] # unpack a list of pairs into two tuples\n",
    "        x = range(1,len(y)+1)\n",
    "        ax.plot(x, y, label = key + ' train', linestyle=\"--\")\n",
    "        y = arun[:,1] # unpack a list of pairs into two tuples\n",
    "        x = range(1,len(y)+1)\n",
    "        ax.plot(x, y, label = key + ' validate', linestyle=\"-\")\n",
    "\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.grid('on')\n",
    "    plt.title(plot_label)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Percent Accuracy\")\n",
    "    plt.legend()    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    lgd = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2), ncol=cols)\n",
    "    plt.savefig(plot_label, bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our data after each test, useful if we need to redo graphs or tables, etc.\n",
    "import pickle\n",
    "\n",
    "def save_data(run_name, res_run):\n",
    "\n",
    "    with open(run_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(res_run, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and build a new chart\n",
    "def graph_pickle(run_name, slots):\n",
    "    with open(run_name + '.pkl', 'rb') as f:\n",
    "        res_run = pickle.load(f)\n",
    "    show_chart(run_name, res_run,slots)\n",
    "    \n",
    "graph_pickle('pre-processing', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######general format of a run, used as my inital test and the template for subsequent runs ####\n",
    "'''\n",
    "#assume data has been loaded\n",
    "\n",
    "#if we need to retokenize\n",
    "train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=True, \n",
    "                                                                        td_toLower=True, \n",
    "                                                                        td_removePunct=True, \n",
    "                                                                        td_ngrams=1) \n",
    "                                                                        \n",
    "\n",
    "#if we want to change vocab size\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = MAX_VOCAB_SIZE)\n",
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "val_data_indices = token2index_dataset(val_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "\n",
    "#if we want to change sentance length or batch size\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENANCE_LENGTH = 400\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets, MAX_SENTENANCE_LENGTH)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets, MAX_SENTENANCE_LENGTH)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets, MAX_SENTENANCE_LENGTH)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Criterion \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#some more hyper paramaters\n",
    "emb_dim = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Create model and optimizer, must redo for any change\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "run_model(verbose=False)\n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs), \": Val Acc {}\".format(test_model(val_loader, model)))\n",
    "#print (\"Test Acc {}\".format(test_model(test_loader, model)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume data has been loaded\n",
    "\n",
    "#iterate over various pre-processing paramaters\n",
    "run_results = {}\n",
    "\n",
    "for number_scheme in [\"nothing\", \"replace\", \"remove\"]:\n",
    "    for lower in [True, False]:\n",
    "        for removePunct in [True, False]:\n",
    "    \n",
    "                replaceNumbers = False\n",
    "                removeNumbers = False\n",
    "\n",
    "                if number_scheme == \"replace\":\n",
    "                    replaceNumbers = True\n",
    "                elif number_scheme == \"remove\":\n",
    "                    removeNumbers = True\n",
    "                    \n",
    "                    \n",
    "                #if we need to retokenize\n",
    "                train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=replaceNumbers, \n",
    "                                                                                        td_removeNumbers=removeNumbers, \n",
    "                                                                                        td_toLower=lower, \n",
    "                                                                                        td_removePunct=removePunct, \n",
    "                                                                                        td_ngrams=1) \n",
    "                                                                                        \n",
    "                #if we want to change vocab size\n",
    "                MAX_VOCAB_SIZE = 20000\n",
    "                token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = MAX_VOCAB_SIZE)\n",
    "                train_data_indices = token2index_dataset(train_tokens)\n",
    "                val_data_indices = token2index_dataset(val_tokens)\n",
    "                test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "\n",
    "                #if we want to change sentance length or batch size\n",
    "                BATCH_SIZE = 32\n",
    "                MAX_SENTENANCE_LENGTH = 200\n",
    "                train_dataset = NewsGroupDataset(train_data_indices, train_targets, MAX_SENTENANCE_LENGTH)\n",
    "                train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "                val_dataset = NewsGroupDataset(val_data_indices, val_targets, MAX_SENTENANCE_LENGTH)\n",
    "                val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "                test_dataset = NewsGroupDataset(test_data_indices, test_targets, MAX_SENTENANCE_LENGTH)\n",
    "                test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                # Criterion \n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "                #some more hyper paramaters\n",
    "                emb_dim = 50\n",
    "                learning_rate = 0.001\n",
    "                num_epochs = 10 # number epoch to train\n",
    "\n",
    "                # Create model and optimizer, must redo for any change\n",
    "                model = BagOfWords(len(id2token), emb_dim)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                res = run_model(verbose=False)\n",
    "\n",
    "                key = \"\"\n",
    "                key += \"remove_numbers-\" if removeNumbers else \"\"\n",
    "                key += \"replace_numbers:\" if replaceNumbers else \"\"\n",
    "                key += \"lowercase\" if lower else \"mix_case\"\n",
    "                key += \"no_punctuation\" if removePunct else \"include_punctuation\"\n",
    "\n",
    "                run_results[key] = res\n",
    "                \n",
    "                print(\"removeNumbers:\", removeNumbers, \" , replaceNumbers:\", replaceNumbers, \" , lower:\", lower, \" , removePunct:\", removePunct, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "    \n",
    "save_data(\"pre-processing\", run_results)\n",
    "show_chart(\"pre-processing\",run_results,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX_VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENANCE_LENGTH = 200\n",
    "emb_dim = 50\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True , train Acc 97.135 , Val Acc 87.48\n",
    "removeNumbers:False , replaceNumbers:False , lower:True , removePunct:False , train Acc 96.195 , Val Acc 87.2\n",
    "removeNumbers:False , replaceNumbers:False , lower:False , removePunct:True , train Acc 97.085 , Val Acc 87.54\n",
    "removeNumbers:False , replaceNumbers:False , lower:False , removePunct:False , train Acc 96.545 , Val Acc 86.9\n",
    "removeNumbers:False , replaceNumbers:True , lower:True , removePunct:True , train Acc 96.945 , Val Acc 87.52\n",
    "removeNumbers:False , replaceNumbers:True , lower:True , removePunct:False , train Acc 96.2 , Val Acc 87.24\n",
    "removeNumbers:False , replaceNumbers:True , lower:False , removePunct:True , train Acc 97.13 , Val Acc 87.04\n",
    "removeNumbers:False , replaceNumbers:True , lower:False , removePunct:False , train Acc 96.46 , Val Acc 86.64\n",
    "removeNumbers:True , replaceNumbers:False , lower:True , removePunct:True , train Acc 96.795 , Val Acc 87.48\n",
    "removeNumbers:True , replaceNumbers:False , lower:True , removePunct:False , train Acc 95.905 , Val Acc 86.94\n",
    "removeNumbers:True , replaceNumbers:False , lower:False , removePunct:True , train Acc 96.87 , Val Acc 87.32\n",
    "removeNumbers:True , replaceNumbers:False , lower:False , removePunct:False , train Acc 96.195 , Val Acc 87.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume data has been loaded\n",
    "\n",
    "#not a wide range of differences for the tokenization, we'll pick the best on the last run \n",
    "#-removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True\n",
    "run_results = {}\n",
    "\n",
    "#iterate over the number of n-grams and vocab size\n",
    "for grams in (1,2,3,4):\n",
    "                    \n",
    "                    \n",
    "    #if we need to retokenize\n",
    "    train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=False, \n",
    "                                                                            td_removeNumbers=False, \n",
    "                                                                            td_toLower=True, \n",
    "                                                                            td_removePunct=True, \n",
    "                                                                            td_ngrams=grams)\n",
    "    \n",
    "    for vocab in (10000, 20000, 30000, 40000, 50000, 100000, 200000):\n",
    "    \n",
    "        token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = vocab)\n",
    "        train_data_indices = token2index_dataset(train_tokens)\n",
    "        val_data_indices = token2index_dataset(val_tokens)\n",
    "        test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "\n",
    "        #if we want to change sentance length or batch size\n",
    "        BATCH_SIZE = 32\n",
    "        MAX_SENTENANCE_LENGTH = 200\n",
    "        train_dataset = NewsGroupDataset(train_data_indices, train_targets, MAX_SENTENANCE_LENGTH)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        val_dataset = NewsGroupDataset(val_data_indices, val_targets, MAX_SENTENANCE_LENGTH)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        test_dataset = NewsGroupDataset(test_data_indices, test_targets, MAX_SENTENANCE_LENGTH)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        # Criterion \n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        #some more hyper paramaters\n",
    "        emb_dim = 50\n",
    "        learning_rate = 0.001\n",
    "        num_epochs = 10 # number epoch to train\n",
    "\n",
    "        # Create model and optimizer, must redo for any change\n",
    "        model = BagOfWords(len(id2token), emb_dim)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        arun = run_model(verbose=False)\n",
    "        run_label = str(grams)+ \"-gram,\" + str(vocab) + \"vocab size\"\n",
    "        \n",
    "        run_results[run_label] = arun\n",
    "        \n",
    "        print(\"n-gram size:\", grams, \" , vocab size:\" , vocab, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "\n",
    "save_data(\"tokenization-vocab\", run_results)\n",
    "show_chart(\"tokenization-vocab\", run_results,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram size:1 , vocab size:10000 , train Acc 94.345 , Val Acc 87.06\n",
    "n-gram size:1 , vocab size:20000 , train Acc 97.22 , Val Acc 87.4\n",
    "n-gram size:1 , vocab size:30000 , train Acc 97.965 , Val Acc 87.38\n",
    "n-gram size:1 , vocab size:40000 , train Acc 98.415 , Val Acc 87.34\n",
    "n-gram size:1 , vocab size:50000 , train Acc 98.645 , Val Acc 87.2\n",
    "n-gram size:1 , vocab size:100000 , train Acc 99.075 , Val Acc 87.56\n",
    "n-gram size:2 , vocab size:10000 , train Acc 92.8 , Val Acc 86.5\n",
    "n-gram size:2 , vocab size:20000 , train Acc 95.58 , Val Acc 86.4\n",
    "n-gram size:2 , vocab size:30000 , train Acc 96.83 , Val Acc 87.44\n",
    "n-gram size:2 , vocab size:40000 , train Acc 97.915 , Val Acc 87.56\n",
    "n-gram size:2 , vocab size:50000 , train Acc 98.35 , Val Acc 87.32\n",
    "n-gram size:2 , vocab size:100000 , train Acc 99.285 , Val Acc 87.44\n",
    "n-gram size:3 , vocab size:10000 , train Acc 93.235 , Val Acc 86.56\n",
    "n-gram size:3 , vocab size:20000 , train Acc 95.395 , Val Acc 87.72\n",
    "n-gram size:3 , vocab size:30000 , train Acc 96.75 , Val Acc 87.32\n",
    "n-gram size:3 , vocab size:40000 , train Acc 97.505 , Val Acc 87.54\n",
    "n-gram size:3 , vocab size:50000 , train Acc 97.845 , Val Acc 87.58\n",
    "n-gram size:3 , vocab size:100000 , train Acc 98.725 , Val Acc 88.0\n",
    "n-gram size:4 , vocab size:10000 , train Acc 93.345 , Val Acc 87.12\n",
    "n-gram size:4 , vocab size:20000 , train Acc 95.52 , Val Acc 87.46\n",
    "n-gram size:4 , vocab size:30000 , train Acc 96.65 , Val Acc 87.06\n",
    "n-gram size:4 , vocab size:40000 , train Acc 97.025 , Val Acc 87.5\n",
    "n-gram size:4 , vocab size:50000 , train Acc 97.54 , Val Acc 87.56\n",
    "n-gram size:4 , vocab size:100000 , train Acc 98.12 , Val Acc 87.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True\n",
    "# n-gram size:3 , vocab size:40000 \n",
    "                    \n",
    "#iterate over different max sentence length values\n",
    "    \n",
    "#if we need to retokenize\n",
    "train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=False, \n",
    "                                                                        td_removeNumbers=False, \n",
    "                                                                        td_toLower=True, \n",
    "                                                                        td_removePunct=True, \n",
    "                                                                        td_ngrams=3)\n",
    "\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 40000)\n",
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "val_data_indices = token2index_dataset(val_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "run_results = {}\n",
    "\n",
    "for slength in [50,100,150,200,250,300,350,400,450,500]:\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    train_dataset = NewsGroupDataset(train_data_indices, train_targets, slength)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    val_dataset = NewsGroupDataset(val_data_indices, val_targets, slength)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    test_dataset = NewsGroupDataset(test_data_indices, test_targets, slength)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Criterion \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    #some more hyper paramaters\n",
    "    emb_dim = 50\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Create model and optimizer, must redo for any change\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    arun = run_model(verbose=False)\n",
    "\n",
    "    reslabel = str(slength) + \" max sentence\"\n",
    "    \n",
    "    run_results[reslabel] = arun\n",
    "    \n",
    "    print(\"Max Sentence Length:\", slength, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "\n",
    "save_data(\"sentence-length\", run_results)\n",
    "show_chart(\"sentence-length\", run_results,2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Sentence Length:50 , train Acc 94.67 , Val Acc 78.34\n",
    "Max Sentence Length:100 , train Acc 97.075 , Val Acc 83.88\n",
    "Max Sentence Length:150 , train Acc 97.97 , Val Acc 86.76\n",
    "Max Sentence Length:200 , train Acc 98.51 , Val Acc 87.58\n",
    "Max Sentence Length:250 , train Acc 98.925 , Val Acc 88.42\n",
    "Max Sentence Length:300 , train Acc 98.965 , Val Acc 89.06\n",
    "Max Sentence Length:350 , train Acc 99.16 , Val Acc 89.24\n",
    "Max Sentence Length:400 , train Acc 99.135 , Val Acc 89.34\n",
    "Max Sentence Length:450 , train Acc 99.185 , Val Acc 89.72\n",
    "Max Sentence Length:500 , train Acc 99.135 , Val Acc 89.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True\n",
    "# n-gram size:3 , vocab size:40000 \n",
    "# Max Sentence Length:500 \n",
    "\n",
    "run_results = {}\n",
    "            \n",
    "'''\n",
    "#if we need to retokenize\n",
    "train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=False, \n",
    "                                                                        td_removeNumbers=False, \n",
    "                                                                         td_toLower=True, \n",
    "                                                                        td_removePunct=True, \n",
    "                                                                        td_ngrams=3)\n",
    "\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 40000)\n",
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "val_data_indices = token2index_dataset(val_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)\n",
    "'''\n",
    "\n",
    "slength = 500\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for emb_dim in (10,25,50,100,150,200):\n",
    "\n",
    "       \n",
    "    train_dataset = NewsGroupDataset(train_data_indices, train_targets, slength)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = NewsGroupDataset(val_data_indices, val_targets, slength)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = NewsGroupDataset(test_data_indices, test_targets, slength)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle=False)\n",
    "\n",
    "    # Criterion \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    #some more hyper paramaters\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Create model and optimizer, must redo for any change\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    arun = run_model(verbose=False)\n",
    "\n",
    "    res_label = str( emb_dim ) + \" embedding size\"\n",
    "    run_results[res_label] = arun\n",
    "    \n",
    "    print(\"Embedding Size:\",  emb_dim, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "\n",
    "save_data(\"embeddingsize\", run_results)\n",
    "show_chart(\"embeddingsize\", run_results,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True\n",
    "# n-gram size:3 , vocab size:40000 \n",
    "# Max Sentence Length:400 \n",
    "\n",
    "#adam\n",
    "\n",
    "run_results = {}\n",
    "            \n",
    "\n",
    "#if we need to retokenize\n",
    "train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=False, \n",
    "                                                                        td_removeNumbers=False, \n",
    "                                                                         td_toLower=True, \n",
    "                                                                        td_removePunct=True, \n",
    "                                                                        td_ngrams=3)\n",
    "\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 40000)\n",
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "val_data_indices = token2index_dataset(val_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "slength = 500\n",
    "batch_size = 32\n",
    "emb_dim = 100\n",
    " \n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets, slength)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets, slength)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets, slength)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle=False)\n",
    "\n",
    "# Criterion \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for learning_rate in (0.5, 0.1, 0.05, 0.01, 0.005, 0.001):\n",
    "    for num_epochs in (2,5,10,15):\n",
    "\n",
    "        #some more hyper paramaters\n",
    "\n",
    "        # Create model and optimizer, must redo for any change\n",
    "        model = BagOfWords(len(id2token), emb_dim)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        arun = run_model(verbose=False, num_epochs = num_epochs)\n",
    "\n",
    "        res_label = str( learning_rate ) + \" learning rate and \" + str(num_epochs) + \" epochs\" \n",
    "        run_results[res_label] = arun\n",
    "\n",
    "        print(\"Learning rate:\",  learning_rate, \" epochs:\", num_epochs, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "\n",
    "save_data(\"learning-epoch\", run_results)\n",
    "show_chart(\"learning-epoch\", run_results,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True\n",
    "# n-gram size:3 , vocab size:40000 \n",
    "# Max Sentence Length:400 \n",
    "\n",
    "#SGD\n",
    "\n",
    "\n",
    "run_results = {}\n",
    "            \n",
    "\n",
    "#if we need to retokenize\n",
    "train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=False, \n",
    "                                                                        td_removeNumbers=False, \n",
    "                                                                         td_toLower=True, \n",
    "                                                                        td_removePunct=True, \n",
    "                                                                        td_ngrams=3)\n",
    "\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 40000)\n",
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "val_data_indices = token2index_dataset(val_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)\n",
    "\n",
    "slength = 500\n",
    "batch_size = 32\n",
    "emb_dim = 100\n",
    " \n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets, slength)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets, slength)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets, slength)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle=False)\n",
    "\n",
    "# Criterion \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for learning_rate in (0.5, 0.1, 0.05, 0.01):\n",
    "    for num_epochs in (10,100,200,400):\n",
    "\n",
    "        #some more hyper paramaters\n",
    "\n",
    "        # Create model and optimizer, must redo for any change\n",
    "        model = BagOfWords(len(id2token), emb_dim)\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        arun = run_model(verbose=False, num_epochs = num_epochs)\n",
    "\n",
    "        res_label = str( learning_rate ) + \" learning rate and \" + str(num_epochs) + \" epochs\" \n",
    "        run_results[res_label] = arun\n",
    "\n",
    "        print(\"Learning rate:\",  learning_rate, \" epochs:\", num_epochs, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "\n",
    "save_data(\"sgd-learning-epoch\", run_results)\n",
    "show_chart(\"sgd-learning-epoch\", run_results,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-removeNumbers:False , replaceNumbers:False , lower:True , removePunct:True\n",
    "# n-gram size:3 , vocab size:40000 \n",
    "# Max Sentence Length:400 \n",
    "\n",
    "#SGD\n",
    "\n",
    "\n",
    "run_results = {}\n",
    "            \n",
    "'''\n",
    "#if we need to retokenize\n",
    "train_tokens, val_tokens, test_tokens, all_train_tokens = tokenize_data(td_replaceNumbers=False, \n",
    "                                                                        td_removeNumbers=False, \n",
    "                                                                         td_toLower=True, \n",
    "                                                                        td_removePunct=True, \n",
    "                                                                        td_ngrams=3)\n",
    "\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 40000)\n",
    "train_data_indices = token2index_dataset(train_tokens)\n",
    "val_data_indices = token2index_dataset(val_tokens)\n",
    "test_data_indices = token2index_dataset(test_tokens)\n",
    "'''\n",
    "slength = 500\n",
    "batch_size = 32\n",
    "emb_dim = 100\n",
    " \n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets, slength)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets, slength)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets, slength)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle=False)\n",
    "\n",
    "# Criterion \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "for learning_rate in (1, 0.5 ):\n",
    "    for num_epochs in (2, 10, 100, 200): \n",
    "\n",
    "        #some more hyper paramaters\n",
    "\n",
    "        # Create model and optimizer, must redo for any change\n",
    "        model = BagOfWords(len(id2token), emb_dim)\n",
    "        #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        lambda1 = lambda epoch: 0.97 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "        \n",
    "        arun = run_model(verbose=False, num_epochs = num_epochs)\n",
    "\n",
    "        res_label = str( learning_rate ) + \" learning rate and \" + str(num_epochs) + \" epochs\" \n",
    "        run_results[res_label] = arun\n",
    "\n",
    "        print(\"Learning rate:\",  learning_rate, \" epochs:\", num_epochs, \" , train Acc {}\".format(test_model(train_loader, model)), \" , Val Acc {}\".format(test_model(val_loader, model)), sep='')\n",
    "\n",
    "save_data(\"sgd-linear-learning-epoch\", run_results)\n",
    "show_chart(\"sgd-linear-learning-epoch\", run_results,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#show some specific results to look at examples\n",
    "#remove shuffle so we can index the inital text\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset=NewsGroupDataset(val_data_indices, val_targets, MAX_SENTENANCE_LENGTH), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "runs = 0\n",
    "for data, lengths, labels in val_loader2:\n",
    "    labels = labels.long()\n",
    "    data_batch, length_batch, label_batch = data, lengths, labels\n",
    "    outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    \n",
    "    for i in range(0,len(labels)):\n",
    "        print( i,predicted[i],\"==\" if predicted[i] == labels[i] else \"!\", labels[i])\n",
    "    \n",
    "    if runs > 1:\n",
    "        break\n",
    "    runs += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view a review\n",
    "i = 27\n",
    "print(i,predicted[i],\"==\", labels[i])\n",
    "print(val_targets[i], val_reviews[i])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
